---
layout: post
title: NLP Music Recommender
---

Today's post is about how I used natural language processing (NLP) to build a music recommendation system. Here's a [link](link) to a webpage where you can play with the recommendation system. My basic idea was to see to what extent the nuance of language would reveal similarities in music not captured by traditional recommendation systems. At the end of the post, I'll give examples of using my model to make recommendations, find musical mashups, and to complete musical analogies. 


There are many websites/apps, including Pandora, Spotify and  Amazon music that recommend music based on the music you like. From what I understand, Amazon's recommendations are generated by finding what other users with purchase history similar to yours also like. Pandora was among the first online recommendation systems, and initially had music experts rate music on several musical features. They now also have a up/down vote which they incorporate. Spotify is an extremely successful business whose recommendation algorithm is surely worth millions of dollars. My idea was to have customers rate music on several musical features, instead of experts. To do this, I used customer reviews of music to extract topics and analyzed the sentiment of each review to give the music a rating on that topic.


I used a large database of amazon reviews (citation) found [here](http://jmcauley.ucsd.edu/data/amazon/). I put the entire CD & Vinyl database into MongoDB on an aws instance. Due to the size of the data set, I decided to subset on reviews in the year 2013 and to CDs with at least 10 reviews that year. The resulting data set consisted of about 130,000 reviews of 6,000 unique CDs. I broke the reviews up in to (500,000+) individual sentences so that one reviewer could assess several independent aspects of the music. Due to the size of the data set, I did all of my work in a Jupyter notebook on an aws instance.


The tools that proved most useful for me were non-negative matrix factorization (NMF) and count vectorizer (CV). Count vectorizer took the 500K sentences and looked at the words that occur in each sentence. I chose an n-gram size of 2, so CV also counted instances of pairs of words, such as "great band". I also chose a threshold of frequency of occurences of a word, so that if a word was too common, it was ignored all together. One wants to exclude common and uninformative words like "great band".


So CV turned the family of sentences in to a bunch of vectors (hence the name), labelled by the number of occurences of each word. We now feed this vectorization in to NMF. The idea behind NMF is to assume that the documents are written about some number of topics, say "cats, dogs, airplanes". We further assume that, each document on each particular topic is a random selection of some words, with some probability distribution. For example, each document in the "cats" topic is written by selecting from the words (cat, meow, sleep, play, eat) with probabilities (.5, .2, .1, .1, .1). So a typical document about cats might be "cat cat meow eat cat". This, of course, is just a silly example. But what NMF does is *make* this assumption, and then try to learn the topics (cats, dogs, airplanes) and the words associated to each topic (cat, meow, sleep, play, eat) along with their probabilities (.5, .2, .1, .1, .1).


I played at great length with the number of topics and the frequency threshold (and other tools such as tfidf, LDA, and PCA) to find the best breakdown of topics. For each iteration, I checked the most frequent words in each topic to get a sense of what the topic was about. In the end I settled on generating 30 topics, some of which I threw out (e.g. those about the amazon delivery, the cost, etc.). I also chose names for each topic based on the words that appeared with the highest probability in each topic. See below for the topic names.


Next, I wanted to give a score for each album and for each topic. Given the NMF model and a sentence, you can generate a probability that the sentence belongs to each topic. So, on the one hand I calculated this probability vector. On the other hand, I used a tool called TextBlob to analyze the sentiment of the sentence. The sentiment is a value from -1 to 1 judging how negative or positive a sentence is. For example, "The absolute worst terrible sad" would receive a score close to -1. Finally, I multiplied the probability vector by the sentiment, to obtain a rating on every topic for the album which the sentence was about. To get one score for the album, I averaged this procedure over all sentences reviewing a given album. Here's an example of an album and its score. You can also see the names I chose for each of the topics.


(image of doors album)


At this point we almost have the recommendation system! Notice what we have done: we have represented each album as a point in a 19 dimensional vector space. If the topics we learned from NMF and the user reviews were meaningful, then albums whose vectors are close to each other should be similar. Rather than finding the nearest vectors using the usual Euclidean distance, I chose to say that two vectors are close if the angle between them is small. The idea here is that music with the same *proportion* of each feature (e.g. rock and folk) are similar, and that the angle captures this closeness of proportionality. 


That's exactly how my recommendation system works! Given an album that you like, I find the 5 albums so that the angle between their vectors and the vector representing your album are smallest. But there's more we can do once we have developed this framework. One thing we could do, for example, is specify the values of each topic we would like and have returned the 5 albums closest to that specification. You can imagine giving a 3 to 1 ratio of folk to rock, for example. I haven't built this in to my web app because it seems pretty cumbersome to enter 19 values...


But you can also add and subtract albums! To make this more meaningful, we should first normalize the albums so that they all have the same length (this preserves the angles between albums) so that the addition is not artificially dominated by one of the albums.  Addition of two albums, then, corresponds to finding the vector halfway between the two albums. Let's see how this plays out in practice using an example. Since we have normalized to unit length, we can represent each album/vector as a pie chart. What do you think will happen when we add "The Best of Miles Davis and John Coltrane" (a jazz album) with "Heart of Gold" by Neil Young (a rock/folk album)?


(image of pie charts added together)


You get soul and gospel music! The closest album is "Soul Sessions" by Joss Stone! Not bad, if you ask me. Not all of the musical mashups created by my model make as much sense, but many of them do. 


Let's take a look at the recommender system in action using pie charts. Below is "Axis: Bold as Love" by Jimi Hendrix, and Aretha Franklin's 30 Greatest Hits Album. It is no mistake that their pie charts look so much alike. The top recommendation based on the Jimi album is the Franklin album. Personally I think this recommendation makes sense. But it is interesting to see which dominate topics Jimi and Franklin have in common. Customers think they both are strong in vocals/bass/drums, artistry/style, pop/jazz, easy/catchy and realness of music. I'm not so sure about the easy/catchy bit, but otherwise I would tend to agree!

(image of jimi and aretha)


As another example, when I entered Radiohead's "OK Computer" the top 5 results were "Best of Depeche Mode 1", "Essential Jimmie Rodgers", "All Over the World: The Very Best of Electric Light Orchestra", "Aja" and "Essential Daryl Hall & John Oates". I think that the only really bizarre recommendation here is Jimmie Rodgers. Depeche Mode is spot on, Electric Light Orchestra and this particular Steely Dan album (Aja) are unconventional but make sense, and the Daryl Hall and John Oates could go either way.


Finally, we can use the vector space structure to **have the model complete analogies for us**. Say we have albums A, B, C and D. Then if A is to B as C is to D, then A - B = C - D. Therefore if we want to finish the analogy A is to B as C is to **blank** we need only calculate C - A + B. This is a fun one: let's try to finish the analogy Kanye West's "College Dropout" is to A Tribe Called Quest's "The Low End Theory" as the Backstreet Boys' "Backstreet Boys" is to **blank**. Any thoughts? Here's what my model comes up with, in order:


"Yoshimi Battles the Pink Robots" by the Flaming Lips, the Bible (oops! should have filtered the Bible out of the data set), and Ken Burns Jazz: The Story of America's Music (The Low End Theory and Backstreet Boys round out the top 5). 


At first I was disappointed, but I listened to both albums a bit, and it sort of makes sense from a musical point of view. The Flaming Lips are easy listening, smooth, and poppy like the Backstreet Boys, but a little bit edgier, more instrumental and less produced. Meanwhile the Ken Burns albums has music by Louis Armstrong, Billie Holiday, Duke Ellington and the like. At the risk of insulting these classic artists, I can imagine them as being predecessor's both temporally and musically to the Backstreet Boys, in much the same way that A Tribe Called Quest preceded Kanye West. They are certainly more instrumental and less produced that the Backstreet Boys, too. Pretty cool!


This exploration answered my original question in the affirmative. Language *is* more nuanced and does surface surprising musical relationship that conventional recommendation engines do not. I am sure that my model is far from perfect, and that some of the errors in my methods led to noise that could be mistakenly and coincidentally perceived as 'the nuance of language'. Nonetheless, I think there is evidence that my model detected some signal coming from the way humans *feel* about music that upvotes and experts alone may miss. For a final comparison, here are three recommender systems and my recommendation, based off of the Axis: Bold as Love album again. 


(image of recommendations)

Pandora and Spotify do a pretty good job, while Amazon is a bore. Overall I would say my recommendations are more exploratory, and less 'safe' than the main stream models. This is due, again, to the imperfections I incidentally built in to the model, as well as the imprecision and the subtlety of language.


That's all from me for now. You can find the source code for this project here (link). Note that the code is seriously computationally intensive and may crash your computer, so be careful!


Soon I hope to have time to go back and blog about my project predicting flight delays using regression. Up next is my final project for my data science boot camp! I'm not sure what I'll be working on yet, but I *am* excited to be out of school (again) and entering the work force. Thanks for reading!