---
layout: post
title: Topological Data Analysis
---

 In this post I will discuss topological data analysis (TDA). First I'll give some intuition about what topology is, and why it is natural to use topology to study data. Then I'll describe two aspects of TDA: submanifold learning and persistent homology.  Note: I tried to write this to be accessible to a non-mathematical audience. The original papers themselves are written by and primarily for a mathematical audience, but the ideas themselves are pretty intuitive. I hope you'll agree! Also, if you come from a more programmatic background, take a look at the [python code](https://github.com/michaelaaroncantrell/michaelaaroncantrell.github.io/blob/master/_posts/submanifold_learning/Submanifold_Learning_Blog.ipynb) I wrote to illustrate the ideas on a simple example (which is also discussed below without code). There is also an R package [TDA](https://cran.r-project.org/web/packages/TDA/index.html) that implements the algorithms below. 

 Topology is a branch of mathematics concerned with shape. The usual joke is that the topologist can't distinguish between her doughnut and her coffee mug. Why? Because they both have the same shape, namely, that of a (solid) torus. One dimension lower, topology doesn't distinguish between a triangle and a circle. They both are connected and have one 'hole'. It is this notion of shape that TDA tries to understand about data. One way to think of the topological perspective is that the topology of an object is invariant under stretching and shrinking, as long as there is no tearing. Intuitively, you can imagine if a doughnut were made out of puddy that you could stretch it into the shape of a coffee mug without tearing it (so you woudln't have changed its topology), but that you could not stretch it in to a sphere without tearing it. The [topology wikipedia page](https://en.wikipedia.org/wiki/Topology) has a graphic illustrating the doughnut and coffee mug. As another example, the letters A and O have the same topology, but B and O do not.

 To motivate why a data scientist might care about the topology of a data set, let me describe another way to arrive at the topological perspective: topology is what's left over when we take the metric away from geometry. In other words, topology remembers that about the object which is insensitive to the particular (compatible) metric representation of the object. For example, the set of all points distance one from the origin (the 'unit ball') in the standard Euclidean metric is the round circle with radius one. If instead we use the [L^1](https://en.wikipedia.org/wiki/Lp_space) or 'Manhattan' metric, the unit ball becomes a diamond. The topology of the unit ball can be thought of as what is common to all of these geometric representations thereof.

A data set often has several feature variables with intrinsically different units. For example, a person's age and their annual income. There is no natural choice of a metric to combine these data. One can (and often does) normalize or scale the data, for example in linear or logistic regression, so that all features have approximately the same weight on the model. But this choice, too, is arbitrary. Why not have one feature have twice as much weight as the other? Moreover, how do we combine the distances between the two features? Using the L^1, L^2, or some other metric? Topology has the nice feature of being independent of these unnatural and arbitrary choices. The topology of the data is exactly that about the shape of the data which is intrinsic and metric-agnostic.

Now that we have a basic understanding of topology, I'll jump in to submanifold learning and persistent homology. But first, what is a manifold? You can think of a manifold  as being a smooth curve, smooth surface, or higher dimensional analog. Now the basic idea behind SL is that our data may actually be living on a lower dimensional manifold than the R^n that we have embedded it in. Moreover, if it is, finding this manifold will both lower the dimension of our data set (reducing its complexity) and give us information about the data. As an example, suppose our data is really coming from a circle, but that we observe a noisy version thereof. Here's a picture of what our data might look like.

![Data Living On A Circle](https://github.com/michaelaaroncantrell/michaelaaroncantrell.github.io/blob/master/_posts/submanifold_learning/images/scatter.png?raw=true)

If we plotted our data and saw this, we would be pretty sure that the data 'lives' on a circle. Note that the circle is 1-dimensional, and that it is (in this case) embedded in 2 dimensional space. Since the data is essentially one dimensional, we can reduce the complexity of our data. Indeed, what we have seen visually is that there is some radius r so that, up to some small noise, x^2+y^2=r^2 for all of our instances (x,y). Therefore we could solve for x, for example, and re-write it in terms of y. We have thus reduced the complexity of our data, and found a relationship between our two features. This is progress!

In this toy case, visualization is enough. But what if our data has 100 features? It will be difficult to visualize, even though it may still lie on a 1-dimensional circle in R^100. Determining whether there is such a lower-dimensional submanifold on which a data set 'lives' is the goal of submanifold learning.

How would we systematically go about trying to determine if our data live on a submanifold? Here's what we'll do: first, pick a threshold parameter. We will later tweak this. But for now let's call it epsilon, and think of it as 0.1. To keep things concrete, let's imagine that our data is as in the picture above. First, for every pair of points, let's check whether they are distance less than epsilon apart. Visually, we're drawing the ball of radius epsilon around every point, and whenever two such balls intersect, we're drawing a line between the center vertices. Next, for every triple of points, let's check if there is a point in the three-fold intersection of their epsilon balls. If there is, then we'll draw in the triangle formed by the three vertices.  If our data lies in higher dimensions, we would continue by finding four-fold intersections of epsilon balls and drawing in the pyramid determined by the four points. We would continue like this through the dimension of the ambient space our data is embedded in.

So, for each value of epsilon, we get some collection of lines, triangles, etc. Below is the result for two values of epsilon for the data set above.

![Triangulated Data, epsilon=0.3](https://github.com/michaelaaroncantrell/michaelaaroncantrell.github.io/blob/master/_posts/submanifold_learning/images/2d-1.png?raw=true)
![Triangulated Data, epsilon=0.4](https://github.com/michaelaaroncantrell/michaelaaroncantrell.github.io/blob/master/_posts/submanifold_learning/images/2d-2.png?raw=true)

In *Finding the homology of submanifolds with high confidence from random samples* P. Niyogi, S. Smale, and S. Weinberger prove that if the data are known to 'live' on a submanifold M with certain curvature constraints, then the result of the above process will have the same number of 'holes' in all dimensions as does M. More precisely, the authors' procedure is to take a larger and larger sample of points N (which you can imagine as being more and more densely clustered) and to take the epsilon parameter to zero. The authors prove that there is some threshold for these parameters beyond which the 'hole' data always coincide. Basically, their assumptions ensure that as epsilon decreases and N increases, their is monotonicity in the holes.

A technique like this is often referred to as 'submanifold learning', because one is able to learn a lot about the underlying manifold (its hole data) by taking sufficiently many samples and implementing the procedure described above. It's worth pointing out that the hole (homology) numbers are algorithmically computable using linear algebra in case the space is constructed from lines, triangles, pyramids, as described above.

What happens when our assumptions do no satisfy those of the paper above? Let's take a closer look at the two images above. In the first, epsilon=0.15 while in the second epsilon=0.2. Notice that, in the first image, there are certain triangles which have formed but are not filled in. In terms of homological data, we regard these as errors in our approximation: the data essentially lie on a circle, which has only one hole. Nevertheless, at this value for our parameter, we are seeing too many holes. The idea, then, would be that we can increase the parameter until the epsilon balls around the points forming the triangle all intersect, so that our procedure fills in the triangle. Indeed many of the triangles get filled in when we increase our parameter from 0.15 to 0.2. But, look! New, unfilled triangles appear! Basically, there were two sides of a triangle whose third side was too long to be caught by 0.15, but was caught by the 0.2 parameter. How can we modify this procedure to determine the 'truth' about the number of holes in the data?

Swoop to the rescue persistent homology! Intuitively, the idea is to only consider the holes which appear at all scales. That is, as epsilon increases, if some holes vanish, forget about them. Similarly, if new holes suddenly appear, ignore them. The holes that persist through all scales are the real ones! 

This descrpition is actually problematic, because even in our case above there will be a threshold at which the 'real' hole corresponding to the circle disappears. Indeed, when epsilon is greater than the diameter of the set, the result will have no homology, because all points will form higher dimensional 'triangles' and be glued together with no possibility of holes. In spite of the fact that there is no rigorous way to determine which interval for the parameter one should consider as being the 'correct' one, by considering the homology at all scales, we have a more complete and more informative picture. Moreover, if there is a relatively large interval on which there is only one persistent homology class (hole), then we might conclude that the underlying manifold has one hole.

Notice that the persistent homology point of view is not dissimilar from the more general topological point of view: by being scale agnostic and considering all scales at once, we uncover more robust information.

Let me wrap up by mentioning a few applications found in the paper mentioned above and in *Topology and Data* by Gunnar Carlsson. For the sake of complexity and dimension reduction, submanifold learning might be useful anywhere the data are unmanageably high-dimensional: for example, graphics, pattern recognition, and biology. There are also situations where one might expect to find a submanifold: for example in a sequence of images taken in motion. The dimension of each image may be massive (corresponding to the number of pixels), but the constraints of rigid Euclidean motion suggest serious dimension reduction potential. For my final example, I have to admit that I may be over interpreting Carlsson's paper, but I believe what he suggests is the following: taking a 1970s Stanford study on diabetes and performing manifold learning, he found that he could distinguish between type 1 and type 2 diabetes. You can imagine how useful that would have been if the two types of diabetes hadn't already been discovered!

That's all for now! Coming up soon: I'll be trying to classify whether a flight will be delayed or not. Thanks for reading!